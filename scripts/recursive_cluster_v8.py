import json
import os
import re
import jieba
import hashlib
import numpy as np
from typing import List, Dict, Set
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from filter_sentence import is_valid_sentence, load_stopwords, clean_text
import argparse 

MAX_RECURSION_DEPTH = 30

MAX_KEYWORDS = 3
MAX_CLASS_NAME_CHARS = 15

RANDOM_STATE = 42
STOPWORDS_PATH = 'cn_hit.txt'  # ä¸­æ–‡åœç”¨è¯è·¯å¾„ï¼ˆå¯ä¿®æ”¹ï¼‰


def is_valid_sentence(text: str) -> bool:
    """åˆ¤æ–­è¯æ¡æ˜¯å¦æœ‰æ•ˆ (ç¤ºä¾‹å®ç°)"""
    return isinstance(text, str) and len(text.strip()) > 5

def load_stopwords(filepath: str) -> Set[str]:
    """åŠ è½½åœç”¨è¯è¡¨ (ç¤ºä¾‹å®ç°)"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            # å‡è®¾åœç”¨è¯æ–‡ä»¶æ¯è¡Œä¸€ä¸ªè¯
            stopwords = set(line.strip() for line in f if line.strip())
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä¸€äº›å¸¸è§çš„è‹±æ–‡åœç”¨è¯
        # common_english_stopwords = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "is", "are", "was", "were", "be", "been", "have", "has", "had", "do", "does", "did", "will", "would", "could", "should", "may", "might", "must", "can"}
        # stopwords.update(common_english_stopwords)
        return stopwords
    except FileNotFoundError:
        print(f"[è­¦å‘Š] åœç”¨è¯æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}ï¼Œå°†ä½¿ç”¨ç©ºåœç”¨è¯è¡¨ã€‚")
        return set()

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡æœ¬ (ç¤ºä¾‹å®ç° - ä¿ç•™ä¸­è‹±æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹ç”¨äºåˆ†è¯)"""
    if not isinstance(text, str):
        return ""
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å•è¯ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼ˆç”¨äºåˆ†è¯ï¼‰
    # å¯æ ¹æ®éœ€è¦è°ƒæ•´æ­£åˆ™è¡¨è¾¾å¼
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-_\.!?;:,]', ' ', text)
    # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ª
    cleaned = re.sub(r'\s+', ' ', cleaned)
    return cleaned.strip()

# ==== åœç”¨è¯åŠ è½½ ====
try:
    stopwords = load_stopwords(STOPWORDS_PATH)
except Exception as e:
    print(f"[é”™è¯¯] åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
    stopwords = set()

# ==== æ”¹è¿›çš„æ··åˆè¯­è¨€åˆ†è¯å™¨ ====
def mixed_tokenizer(text: str) -> List[str]:
    """
    æ”¯æŒä¸­è‹±æ–‡çš„åˆ†è¯å™¨ã€‚
    """
    if not isinstance(text, str):
        return []
    tokens = []
    
    # --- å¤„ç†ä¸­æ–‡ ---
    chinese_parts = re.findall(r'[\u4e00-\u9fa5]+', text)
    for part in chinese_parts:
        # ä½¿ç”¨ jieba åˆ†è¯
        chinese_tokens = jieba.cut(part)
        # è¿‡æ»¤åœç”¨è¯å’Œé•¿åº¦å°äº2çš„è¯
        filtered_chinese = [word for word in chinese_tokens if word not in stopwords and len(word) > 1]
        tokens.extend(filtered_chinese)

    # --- å¤„ç†è‹±æ–‡ ---
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è‹±æ–‡å•è¯ (è€ƒè™‘æ’‡å·ï¼Œå¦‚ don't)
    english_parts = re.findall(r"[a-zA-Z]+(?:'[a-zA-Z]+)?", text)
    for part in english_parts:
        word = part.lower() # è½¬ä¸ºå°å†™
        # è¿‡æ»¤åœç”¨è¯å’Œé•¿åº¦å°äº2çš„è¯
        if word not in stopwords and len(word) > 1:
            tokens.append(word)
            
    # --- å¯é€‰ï¼šå¤„ç†æ•°å­— ---
    # number_parts = re.findall(r'\d+', text)
    # tokens.extend(number_parts) # æˆ–è€…æ ¹æ®éœ€è¦å¤„ç†æ•°å­—

    return tokens

def chinese_tokenizer(text):
    return [word for word in jieba.cut(text) if word not in stopwords and len(word) > 1]


# ==== ç±»åç”Ÿæˆ ====
def generate_class_name(texts: List[str], method: str = 'tfidf', parent_name: str = "") -> str:
    texts = [t for t in texts if is_valid_sentence(t)]
    fallback_base = "å­ä¸»é¢˜"
    if not texts:
        return fallback_base
    try:
        cleaned_texts = [clean_text(t) for t in texts]
        # è¿‡æ»¤æ‰æ¸…æ´—åä¸ºç©ºçš„æ–‡æœ¬
        cleaned_texts = [t for t in cleaned_texts if t.strip()]
        if not cleaned_texts:
             raise ValueError("æ¸…æ´—åæ— æœ‰æ•ˆæ–‡æœ¬")

        if method == 'tfidf':
            vectorizer = TfidfVectorizer(max_features=1000, tokenizer=mixed_tokenizer)
            X = vectorizer.fit_transform(cleaned_texts)
            indices = np.argsort(np.asarray(X.sum(axis=0)).ravel())[::-1]

            # åªä¿ç•™ä¸­æ–‡å…³é”®è¯ï¼Œæœ€å¤šå–å‰ MAX_KEYWORDS ä¸ª
            keywords = [
                vectorizer.get_feature_names_out()[i]
                for i in indices
                if re.search(r'[\u4e00-\u9fa5]', vectorizer.get_feature_names_out()[i])
            ][:MAX_KEYWORDS]

            if not keywords or len(keywords) < 1:
                raise ValueError("æ— æœ‰æ•ˆä¸­æ–‡å…³é”®è¯")

            class_segment = "/".join(keywords)

        elif method == 'center':
            vectorizer = TfidfVectorizer(tokenizer=mixed_tokenizer, max_features=1000)
            X = vectorizer.fit_transform(cleaned_texts)
            if X.shape[1] == 0:
                 raise ValueError("TF-IDF å‘é‡åŒ–åæ— ç‰¹å¾")
            sim_matrix = cosine_similarity(X)
            scores = sim_matrix.sum(axis=1)
            center_text = cleaned_texts[np.argmax(scores)]
            class_segment = truncate_text(center_text, MAX_CLASS_NAME_CHARS)
        elif method == 'gen':
            raise NotImplementedError("å°šæœªå®ç°ç”Ÿæˆå¼ç±»å")
        else:
            raise ValueError("æœªçŸ¥å‘½åæ–¹æ³•")
            
        # --- ç¡®ä¿ç±»åé•¿åº¦ç¬¦åˆè¦æ±‚ ---
        return truncate_text(class_segment, MAX_CLASS_NAME_CHARS)

    except Exception as e:
        # print(f"[è°ƒè¯•] generate_class_name å¤±è´¥: {e}") # å¯é€‰è°ƒè¯•ä¿¡æ¯
        hash_suffix = hashlib.md5(" ".join(texts).encode()).hexdigest()[:4]
        return f"{fallback_base}{hash_suffix}"

def truncate_text(text: str, max_len: int) -> str:
    """
    æˆªæ–­æ–‡æœ¬ï¼Œåªä¿ç•™ä¸­æ–‡å­—ç¬¦å’Œ '/'ï¼Œå¹¶é™åˆ¶æœ€å¤§é•¿åº¦ã€‚
    """
    if not isinstance(text, str):
        return ""
    # åªä¿ç•™ä¸­æ–‡å­—ç¬¦å’Œ '/'  (å‡è®¾ '/' æ˜¯æ‚¨ç”¨æ¥åˆ†éš”å…³é”®è¯çš„ç‰¹å®šå­—ç¬¦)
    cleaned_text = re.sub(r'[^\u4e00-\u9fa5/]', '', text) 
    # æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
    truncated = cleaned_text
    return truncated.strip()

# ==== èšç±»é€»è¾‘ ====
def cluster_texts(texts: List[str], max_per_cluster: int = 20) -> Dict[str, List[int]]:
    if len(texts) <= max_per_cluster:
        return {'all': list(range(len(texts)))}
    
    cleaned_texts = [clean_text(t) for t in texts]
    # è¿‡æ»¤æ‰æ¸…æ´—åä¸ºç©ºçš„æ–‡æœ¬
    cleaned_texts = [t for t in cleaned_texts if t.strip()]
    
    # æ–°å¢åˆ¤æ–­ï¼šå¦‚æœæ¸…æ´—åå…¨æ˜¯ç©ºçš„ï¼Œå°±ä¸èšç±»
    if not cleaned_texts or all(t == '' for t in cleaned_texts):
        return {'all': list(range(len(texts)))}
        
    try:
        k = max(2, len(texts) // max_per_cluster)
        # æ·»åŠ  max_features é™åˆ¶å‘é‡ç»´åº¦
        vectorizer = TfidfVectorizer(tokenizer=mixed_tokenizer, max_features=5000) 
        X = vectorizer.fit_transform(cleaned_texts)
        if X.shape[1] == 0: # å¦‚æœæ²¡æœ‰ç‰¹å¾
            return {'all': list(range(len(texts)))}
            
        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10) # å¢åŠ  n_init ä»¥æé«˜ç¨³å®šæ€§
        labels = kmeans.fit_predict(X)
        clusters = {}
        for idx, label in enumerate(labels):
            clusters.setdefault(str(label), []).append(idx)
        return clusters
    except ValueError as ve:
        print(f"[è°ƒè¯•] KMeans ValueError: {ve}") # å¯é€‰è°ƒè¯•ä¿¡æ¯
        return {'all': list(range(len(texts)))}
    except Exception as e:
        print(f"[è°ƒè¯•] cluster_texts å¤±è´¥: {e}") # å¯é€‰è°ƒè¯•ä¿¡æ¯
        return {'all': list(range(len(texts)))}

def recursive_cluster_numbered(
    data_list: List[Dict],
    root_name: str,
    prefix_path: str,
    full_key_path: str,
    depth: int = 1,
    filtered_log=None
) -> Dict:
    if depth > MAX_RECURSION_DEPTH:
        return {f"{prefix_path}.{root_name}_æ·±åº¦è¶…é™": data_list}
        
    valid_items = []
    valid_indices = []
    for idx, item in enumerate(data_list):
        text = item.get("text", "")
        if is_valid_sentence(text):
            valid_items.append(item)
            valid_indices.append(idx)
        else:
            if filtered_log is not None:
                filtered_log.append(item)
                
    if not valid_items:
        return {f"{prefix_path}.{root_name}_æ— æœ‰æ•ˆæ–‡æœ¬": data_list}
        
    texts = [item["text"] for item in valid_items]
    clusters = cluster_texts(texts, MAX_CLUSTER_SIZE)
    result = {}
    name_set = set()
    count = 1
    
    for label, local_indices in clusters.items():
        global_indices = [valid_indices[i] for i in local_indices]
        sub_items = [data_list[i] for i in global_indices]
        sub_texts = [item["text"] for item in sub_items]
        
        cluster_name = generate_class_name(sub_texts, method=CLASS_NAME_METHOD, parent_name="")
        orig_name = cluster_name
        suffix = 1
        while cluster_name in name_set:
            cluster_name = f"{orig_name}_{suffix}"
            suffix += 1
        name_set.add(cluster_name)
        
        new_prefix = f"{prefix_path}-{count}" if prefix_path else str(count)
        key = f"{new_prefix}.{root_name}_{cluster_name}"
        
        # å®‰å…¨é™åˆ¶ key é•¿åº¦
        if len(key) > 250:
            key = key[:250] + "_trunc"
            
        # --- è°ƒæ•´é€’å½’ç»ˆæ­¢æ¡ä»¶ ---
        # åŸºæœ¬åœæ­¢æ¡ä»¶ï¼šç°‡å¤§å°è¶³å¤Ÿå°
        if len(sub_items) <= MAX_CLUSTER_SIZE:
            result[key] = sub_items
        # ç‰¹æ®Šåœæ­¢æ¡ä»¶ï¼šå¦‚æœèšç±»ç»“æœä¸ä½³ï¼ˆä¾‹å¦‚æ‰€æœ‰ç‚¹åœ¨ä¸€ä¸ªç°‡ï¼‰ï¼Œé¿å…æ— é™é€’å½’
        elif len(clusters) <= 1:
             # å¦‚æœåªæœ‰ä¸€ä¸ªç°‡ï¼Œä½†æ•°æ®é‡è¿œå¤§äº MAX_CLUSTER_SIZEï¼Œåˆ™å¼ºåˆ¶å†åˆ†ä¸€æ¬¡ï¼ˆéœ€è¦è°ƒæ•´ kï¼‰
             # æˆ–è€…ç®€å•åœ°ä½œä¸ºå¶å­èŠ‚ç‚¹åœæ­¢ï¼Œé¿å…æ­»å¾ªç¯
             # è¿™é‡Œé€‰æ‹©ä½œä¸ºå¶å­èŠ‚ç‚¹åœæ­¢
             result[key] = sub_items
        else:
            # æ­£å¸¸é€’å½’
            sub_result = recursive_cluster_numbered(
                sub_items,
                root_name=f"{root_name}_{cluster_name}",
                prefix_path=new_prefix,
                full_key_path=key,
                depth=depth + 1,
                filtered_log=filtered_log
            )
            # å¦‚æœé€’å½’è¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆåŒ…å«å­ç°‡ï¼‰ï¼Œåˆ™å±•å¼€ï¼›å¦åˆ™ç›´æ¥èµ‹å€¼
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘å–å†³äº recursive_cluster_numbered çš„è¿”å›ç»“æ„
            # å½“å‰å®ç°ä¸­ï¼Œå®ƒæ€»æ˜¯è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œæ‰€ä»¥ç›´æ¥èµ‹å€¼å³å¯
            result[key] = sub_result 
        count += 1
    return result

# ==== è¾“å‡ºç»“æ„ ====
def print_class_tree(tree: Dict, indent: str = "", level: int = 0):
    for class_name, content in tree.items():
        prefix = "ğŸ“‚" if isinstance(content, dict) else "â”œâ”€â”€"
        print(f"{indent}{prefix} {class_name}")
        if isinstance(content, dict):
            print_class_tree(content, indent + "  ", level + 1)

def build_class_tree_dict(tree: Dict) -> Dict:
    result = {}
    for class_name, content in tree.items():
        if isinstance(content, dict):
            result[class_name] = build_class_tree_dict(content)
        else:
            # å¦‚æœ content æ˜¯åˆ—è¡¨ï¼ˆå¶å­èŠ‚ç‚¹ï¼‰ï¼Œå¯ä»¥è¿”å› None æˆ–å…¶ä»–æ ‡è¯†
            result[class_name] = None 
    return result


from sklearn.cluster import AgglomerativeClustering
def hierarchical_cluster_numbered(
    data_list: List[Dict],
    root_name: str,
    prefix_path: str,
    full_key_path: str,
    max_cluster_size: int = 20,
    filtered_log=None
) -> Dict:
    """
    ä½¿ç”¨å‡èšå¼å±‚æ¬¡èšç±»ï¼ˆAgglomerative Clusteringï¼‰å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å±‚èšç±»ã€‚
    """
    valid_items = []
    valid_indices = []
    for idx, item in enumerate(data_list):
        text = item.get("text", "")
        if is_valid_sentence(text):
            valid_items.append(item)
            valid_indices.append(idx)
        else:
            if filtered_log is not None:
                filtered_log.append(item)

    if not valid_items:
        return {f"{prefix_path}.{root_name}_æ— æœ‰æ•ˆæ–‡æœ¬": data_list}

    texts = [item["text"] for item in valid_items]
    n_samples = len(texts)

    if n_samples <= max_cluster_size:
        return {f"{prefix_path}.{root_name}_å•ç°‡": data_list}

    # æ¸…æ´—å¹¶å‘é‡åŒ–
    cleaned_texts = [clean_text(t) for t in texts]
    cleaned_texts = [t for t in cleaned_texts if t.strip()]
    if not cleaned_texts or all(t == '' for t in cleaned_texts):
        return {f"{prefix_path}.{root_name}_æ— ç‰¹å¾": data_list}

    try:
        vectorizer = TfidfVectorizer(tokenizer=mixed_tokenizer, max_features=5000)
        X = vectorizer.fit_transform(cleaned_texts)
        if X.shape[1] == 0:
            return {f"{prefix_path}.{root_name}_æ— å‘é‡": data_list}

        # è®¡ç®—æœ€å¤§èšç±»æ•°ï¼šè‡³å°‘æ¯ä¸ªç°‡ä¸è¶…è¿‡ max_cluster_size
        max_k = max(2, n_samples // max_cluster_size)
        n_clusters = max_k

        # ä½¿ç”¨å‡èšå¼èšç±»
        clustering_model = AgglomerativeClustering(
            n_clusters=n_clusters,
            metric='cosine',
            linkage='average',
            compute_distances=True  # ä¾¿äºå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
        )
        # æ³¨æ„ï¼šAgglomerativeClustering ä¸æ”¯æŒç¨€ç–çŸ©é˜µï¼Œéœ€è½¬ä¸º dense
        X_dense = X.toarray()
        labels = clustering_model.fit_predict(X_dense)

        clusters = {}
        for idx, label in enumerate(labels):
            clusters.setdefault(str(label), []).append(idx)

        # ç”Ÿæˆèšç±»æ ‘ç»“æ„ï¼ˆä¸ recursive ç»“æ„ä¸€è‡´ï¼‰
        result = {}
        name_set = set()
        count = 1

        for label, local_indices in clusters.items():
            global_indices = [valid_indices[i] for i in local_indices]
            sub_items = [data_list[i] for i in global_indices]
            sub_texts = [item["text"] for item in sub_items]

            cluster_name = generate_class_name(sub_texts, method=CLASS_NAME_METHOD, parent_name="")
            orig_name = cluster_name
            suffix = 1
            while cluster_name in name_set:
                cluster_name = f"{orig_name}_{suffix}"
                suffix += 1
            name_set.add(cluster_name)

            new_prefix = f"{prefix_path}-{count}" if prefix_path else str(count)
            key = f"{new_prefix}.{root_name}_{cluster_name}"

            # é™åˆ¶ key é•¿åº¦
            if len(key) > 250:
                key = key[:250] + "_trunc"

            # å¦‚æœå­ç°‡è¶³å¤Ÿå°ï¼Œä½œä¸ºå¶å­èŠ‚ç‚¹ï¼›å¦åˆ™ç»§ç»­èšç±»ï¼ˆå¯é€‰ï¼šé€’å½’å±‚æ¬¡èšç±»ï¼‰
            # å¦‚æœä½ æƒ³åšå®Œæ•´æ ‘çŠ¶å±‚æ¬¡èšç±»ï¼Œå¯é€’å½’è°ƒç”¨ hierarchical_cluster_numbered
            if len(sub_items) <= max_cluster_size:
                result[key] = sub_items
            else:
                # å¯é€‰ï¼šé€’å½’å±‚æ¬¡èšç±» â†’ æ„å»ºæ›´æ·±çš„æ ‘
                sub_result = hierarchical_cluster_numbered(
                    sub_items,
                    root_name=f"{root_name}_{cluster_name}",
                    prefix_path=new_prefix,
                    full_key_path=key,
                    max_cluster_size=max_cluster_size,
                    filtered_log=filtered_log
                )
                result[key] = sub_result
            count += 1

        return result

    except Exception as e:
        print(f"[è°ƒè¯•] å±‚æ¬¡èšç±»å¤±è´¥: {e}")
        return {f"{prefix_path}.{root_name}_èšç±»å¤±è´¥": data_list}

# ==== ä¸»ç¨‹åº ====
def process_json_file(input_path: str, output_path: str, clustering_mode: str = "recursive"):
    filtered_log = []
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    except FileNotFoundError:
        print(f"[é”™è¯¯] è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {input_path}")
        return
    except json.JSONDecodeError as e:
        print(f"[é”™è¯¯] JSON è§£ç å¤±è´¥: {input_path}, {e}")
        return

    final_data = {}
    for orig_label, items in raw_data.items():
        if "." in orig_label:
            prefix_num, root_name = orig_label.split(".", 1)
        else:
            prefix_num, root_name = "1", orig_label

        print(f"[å¤„ç†] å¼€å§‹èšç±»å¤§ç±»: {orig_label}")

        if clustering_mode == "recursive":
            cluster_result = recursive_cluster_numbered(
                items,
                root_name=root_name,
                prefix_path=prefix_num,
                full_key_path="",
                filtered_log=filtered_log
            )
        elif clustering_mode == "hierarchical":
            cluster_result = hierarchical_cluster_numbered(
                items,
                root_name=root_name,
                prefix_path=prefix_num,
                full_key_path="",
                max_cluster_size=MAX_CLUSTER_SIZE,
                filtered_log=filtered_log
            )
        else:
            print(f"[é”™è¯¯] ä¸æ”¯æŒçš„èšç±»æ¨¡å¼: {clustering_mode}")
            return

        final_data[orig_label] = cluster_result

    print("\n=== ç±»åèšç±»ç»“æ„æ ‘ ===")
    print_class_tree(final_data)

    # ä¿å­˜ç»“æœï¼ˆç•¥ï¼‰
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        print(f"[å®Œæˆ] å·²ä¿å­˜èšç±»ç»“æœåˆ°: {output_path}")
    except Exception as e:
        print(f"[é”™è¯¯] ä¿å­˜èšç±»ç»“æœå¤±è´¥: {output_path}, {e}")

    tree_dict = build_class_tree_dict(final_data)
    tree_path = os.path.splitext(output_path)[0] + "_tree.json"
    try:
        with open(tree_path, 'w', encoding='utf-8') as f:
            json.dump(tree_dict, f, indent=2, ensure_ascii=False)
        print(f"[å®Œæˆ] å·²ä¿å­˜èšç±»æ ‘ç»“æ„åˆ°: {tree_path}")
    except Exception as e:
        print(f"[é”™è¯¯] ä¿å­˜èšç±»æ ‘ç»“æ„å¤±è´¥: {tree_path}, {e}")

    if filtered_log:
        filtered_path = os.path.splitext(output_path)[0] + "_filtered_out.json"
        try:
            with open(filtered_path, 'w', encoding='utf-8') as f:
                json.dump(filtered_log, f, ensure_ascii=False, indent=2)
            print(f"[æç¤º] å·²ä¿å­˜è¢«è¿‡æ»¤æ‰çš„æ— æ•ˆæ–‡æœ¬ï¼š{filtered_path}")
        except Exception as e:
            print(f"[é”™è¯¯] ä¿å­˜è¿‡æ»¤æ—¥å¿—å¤±è´¥: {filtered_path}, {e}")


# ==== æ‰§è¡Œ ====
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é€’å½’æˆ–å±‚æ¬¡èšç±»è„šæœ¬")
    parser.add_argument("--input", required=True, help="è¾“å…¥JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--method", default="tfidf", choices=["tfidf", "center", "gen"], help="ç±»åç”Ÿæˆæ–¹æ³•")
    parser.add_argument("--max_cluster_size", type=int, default=20, help="æ¯ä¸ªèšç±»æœ€å¤§è¯æ¡æ•°ï¼ˆé»˜è®¤: 20ï¼‰")
    parser.add_argument("--clustering-mode", default="recursive", choices=["recursive", "hierarchical"], 
                        help="èšç±»æ¨¡å¼ï¼šrecursiveï¼ˆé€’å½’KMeansï¼‰æˆ– hierarchicalï¼ˆå‡èšå¼å±‚æ¬¡èšç±»ï¼‰")

    args = parser.parse_args()

    # è®¾ç½®å…¨å±€å˜é‡
    global CLASS_NAME_METHOD, MAX_CLUSTER_SIZE
    CLASS_NAME_METHOD = args.method
    MAX_CLUSTER_SIZE = args.max_cluster_size

    process_json_file(args.input, args.output, clustering_mode=args.clustering_mode)