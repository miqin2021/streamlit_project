# cluster-v8.py
import os
import json
import re
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from openai import OpenAI
import argparse  
import time  # æ–°å¢ï¼šç”¨äºé‡è¯•å»¶è¿Ÿ
OUTPUT_DIR = "data/outputs"
OUTPUT_DIR_4 = os.path.join(OUTPUT_DIR, "step4")

# ==== é…ç½®å‚æ•° ====
JSON_DIR = "data/json_layout"
OUTPUT_FILE = os.path.join(OUTPUT_DIR_4, "output-cluster-step4.json")
BAD_PARAGRAPH_LOG = os.path.join(OUTPUT_DIR_4, "output-cluster-step4-ignored-paragraphs.txt")
USE_TFIDF_KEYWORDS = True
ALL_TEXTS_JSON = os.path.join(OUTPUT_DIR_4, "all_paragraph_texts.json")

CONFIG_FILE = "llm.config"

# === å…³é”®ä¿®æ”¹ï¼šæ›´æ–°å‘½ä»¤è¡Œå‚æ•°è§£æï¼Œæ·»åŠ --selected_files ===
def parse_args():
    parser = argparse.ArgumentParser(description="æ‰§è¡Œæ–‡æœ¬èšç±»")
    parser.add_argument(
        "--n_clusters",
        type=int,
        default=9,
        help="èšç±»æ•°é‡ï¼Œå³è¦åˆ†æˆå‡ ç±» (é»˜è®¤: 9)"
    )
    # æ–°å¢ï¼šæ¥æ”¶é€‰ä¸­çš„JSONæ–‡ä»¶åï¼ˆé€—å·åˆ†éš”ï¼‰
    parser.add_argument(
        "--selected_files",
        type=str,
        required=True,
        help="Step3é€‰ä¸­çš„JSONæ–‡ä»¶åï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”"
    )
    return parser.parse_args()

# è¯»å–å‘½ä»¤è¡Œå‚æ•°
args = parse_args()
N_CLUSTERS = args.n_clusters  
# å…³é”®ä¿®æ”¹ï¼šè§£æé€‰ä¸­çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”â†’åˆ—è¡¨ï¼‰
SELECTED_JSON_FILES = args.selected_files.split(",")
print(f"ğŸ“Œ ç›®æ ‡ç±»åˆ«æ•°ï¼š{N_CLUSTERS}")
print(f"ğŸ“Œ å¾…å¤„ç†æ–‡ä»¶æ•°ï¼š{len(SELECTED_JSON_FILES)}")
for idx, file in enumerate(SELECTED_JSON_FILES, 1):
    print(f"  {idx}. {file}")

# ==== OpenAI / ModelScope é…ç½®ï¼ˆåŸæœ‰ä»£ç ä¸å˜ï¼‰ ====
with open(CONFIG_FILE, "r", encoding="utf-8") as f:
    llm_config = json.load(f)

client = OpenAI(
    base_url=llm_config["base_url"],
    api_key=llm_config["api_key"]
)
LLM_MODEL = llm_config.get("model_name", "Qwen/Qwen2.5-72B-Instruct")
TEMPERATURE = float(llm_config.get("temperature", 0.5))  # é»˜è®¤æ¸©åº¦

# ========== å·¥å…·å‡½æ•°ï¼ˆis_valid_sentenceã€split_sentencesã€extract_sentences_from_jsonï¼‰ä¿æŒä¸å˜ ==========
def is_valid_sentence(sentence, lang="zh"):
    if not sentence or len(sentence) < 10:
        return False
    if lang == "zh":
        if re.search(r'[ã€‚ï¼ï¼Ÿ!?]("|â€)?$', sentence):
            return True
        if re.search(r'["â€œâ€].+[ã€‚ï¼ï¼Ÿ!?]["â€œâ€]$', sentence):
            return True
    else:
        if sentence[-1] in '.!?':
            return True
    return False

def split_sentences(text, lang="zh"):
    text = re.sub(r"\s+", " ", text)
    if lang == "zh":
        parts = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?])", text)
        sentences = []
        buffer = ""
        for part in parts:
            buffer += part
            if part.endswith(("ã€‚", "ï¼", "ï¼Ÿ", "!", "?")):
                sentences.append(buffer)
                buffer = ""
        if buffer:
            sentences.append(buffer)
    else:
        sentences = re.split(r'(?<=[.!?])\s+', text)
    return [
        s.strip() for s in sentences
        if isinstance(s, str) and s.strip() and is_valid_sentence(s.strip(), lang)
    ]

def extract_sentences_from_json(json_path, discarded_log):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    results = []
    for page in data.get("pdf_info", []):
        page_num = page.get("page_idx", 0) + 1
        for block in page.get("para_blocks", []):
            if block.get("type") == "text":
                lines = ["".join(span.get("content", "") for span in line.get("spans", [])) for line in block.get("lines", [])]
                paragraph = " ".join(lines).strip()
                if not paragraph:
                    continue
                is_chinese = any('\u4e00' <= ch <= '\u9fff' for ch in paragraph)
                lang = "zh" if is_chinese else "en"
                sentences = split_sentences(paragraph, lang=lang)
                if not sentences:
                    discarded_log.append({
                        "reason": "no valid sentence",
                        "text": paragraph,
                        "source": os.path.basename(json_path),
                        "page": page_num
                    })
                    continue
                for sent in sentences:
                    results.append({
                        "text": sent,
                        "page": page_num,
                        "bbox": block.get("bbox"),
                        "source": os.path.basename(json_path)
                    })
    return results

# ========== å…³é”®ä¿®æ”¹ï¼šä»…å¤„ç†SELECTED_JSON_FILESä¸­çš„æ–‡ä»¶ ==========
print("æ­£åœ¨æå–æ®µè½å¹¶æ‹†åˆ†è¯æ¡ ...")
all_paras = []
ignored_paragraphs = []

# æ›¿æ¢åŸæœ‰çš„â€œéå†JSON_DIRâ€é€»è¾‘ï¼Œæ”¹ä¸ºå¤„ç†é€‰ä¸­çš„æ–‡ä»¶
for file in SELECTED_JSON_FILES:
    # 1. æ¸…ç†æ–‡ä»¶åï¼ˆé¿å…ç©ºæ ¼/ç‰¹æ®Šå­—ç¬¦é—®é¢˜ï¼‰
    file = file.strip()
    # 2. æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼ˆå¿…é¡»æ˜¯JSONï¼‰
    if not file.endswith(".json"):
        print(f"âš ï¸ è·³è¿‡éJSONæ–‡ä»¶ï¼š{file}")
        continue
    # 3. æ‹¼æ¥å®Œæ•´è·¯å¾„
    json_path = os.path.join(JSON_DIR, file)
    # 4. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{json_path}")
        continue
    # 5. æå–æ®µè½ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
    print(f"ğŸ” æ­£åœ¨å¤„ç†ï¼š{file}")
    all_paras.extend(extract_sentences_from_json(json_path, ignored_paragraphs))

# ========== æ–°å¢ï¼šæå–æ‰€æœ‰textå¹¶ä¿å­˜åˆ°JSON ==========
if all_paras:  # ä»…å½“æœ‰æœ‰æ•ˆæ–‡æœ¬æ—¶æ‰ä¿å­˜
    # æ–¹æ¡ˆ2ï¼šä¿å­˜æ–‡æœ¬åŠå…ƒæ•°æ®
    all_texts = [
        {
            "text": para["text"],
            "source": para["source"],
            "page": para["page"]
        } 
        for para in all_paras
    ]

    # å†™å…¥JSONæ–‡ä»¶ï¼ˆæ”¾åœ¨å¾ªç¯å¤–ï¼Œç¡®ä¿åªå†™ä¸€æ¬¡ï¼‰
    with open(ALL_TEXTS_JSON, "w", encoding="utf-8") as f:
        json.dump(all_texts, f, ensure_ascii=False, indent=2)
    print(f"æ‰€æœ‰æœ‰æ•ˆæ–‡æœ¬å·²ä¿å­˜è‡³: {ALL_TEXTS_JSON}")
else:
    print("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œä¸ç”ŸæˆJSONæ–‡ä»¶")


# ========== è¾“å‡ºè·³è¿‡çš„æ®µè½ ==========
if ignored_paragraphs:
    with open(BAD_PARAGRAPH_LOG, "w", encoding="utf-8") as f:
        for item in ignored_paragraphs:
            path = item.get("source", "unknown")
            page = item.get("page", "-")
            para = item.get("text", "")
            f.write(f"{path} - Page {page}\n{para}\n\n")
    print(f"å·²è·³è¿‡æ— æ•ˆæ®µè½ {len(ignored_paragraphs)} æ¡ï¼Œè®°å½•è§ï¼š{BAD_PARAGRAPH_LOG}")

# ========== å‘é‡åŒ– ==========
texts = [p["text"] for p in all_paras]
if not texts:
    print("æ— æœ‰æ•ˆæ–‡æœ¬ï¼Œç¨‹åºç»ˆæ­¢")
    exit()

print("æ­£åœ¨æ‰§è¡Œèšç±» ...")
from modelscope.hub.snapshot_download import snapshot_download
model_dir = snapshot_download("BAAI/bge-m3")
model = SentenceTransformer(model_dir, device="cuda")
embeddings = model.encode(texts, show_progress_bar=True)

# ========== KMeans èšç±» ==========
kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0)
labels = kmeans.fit_predict(embeddings)

# ========== èšç±»æ”¶é›† ==========
cluster_texts = defaultdict(list)
cluster_sources = defaultdict(set)
for i, label in enumerate(labels):
    cluster_texts[label].append(texts[i])
    cluster_sources[label].add(all_paras[i]["source"])

# ========== ä½¿ç”¨ LLM å‘½å ==========
def generate_cluster_label(label, paras, sources, client, LLM_MODEL, TEMPERATURE):
    """ç”Ÿæˆèšç±»ç±»åï¼ˆå«é‡è¯•æœºåˆ¶ï¼Œå…¼å®¹æ ·æœ¬ä¸è¶³5æ¡çš„æƒ…å†µï¼‰"""
    max_retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    max_samples = 5  # æœ€å¤§æ ·æœ¬æ•°é‡
    
    for retry in range(max_retries):
        try:
            # 1. å‡†å¤‡æ ·æœ¬æ•°æ®ï¼ˆåŠ¨æ€é€‚é…å®é™…æ•°é‡ï¼Œæœ€å¤š5æ¡ï¼‰
            actual_samples = min(max_samples, len(paras))  # å–å®é™…æ•°é‡å’Œæœ€å¤§æ•°é‡çš„è¾ƒå°å€¼
            sample_texts = paras[:actual_samples]  # æ ¹æ®å®é™…æ•°é‡æˆªå–æ ·æœ¬
            sources_str = ", ".join(list(sources)[:3])  # å–å‰3ä¸ªæ¥æº
            
            # 2. æ„å»ºåŠ¨æ€æç¤ºè¯ï¼ˆæ ¹æ®å®é™…æ ·æœ¬æ•°é‡è°ƒæ•´æè¿°ï¼‰
            prompt = f"""ä½ æ˜¯ä¸­æ–‡æ–‡æœ¬ä¸»é¢˜å‘½åä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹{actual_samples}æ¡æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä¸ªç²¾å‡†ä¸”ç‹¬ç‰¹çš„ç±»åï¼š
è¦æ±‚ï¼š
- ä¸è¶…è¿‡6ä¸ªæ±‰å­—
- é£æ ¼å­¦æœ¯ã€ä¸­æ€§ï¼Œé¿å…å£è¯­/ç½‘ç»œè¯
- ç²¾å‡†åæ˜ æ–‡æœ¬æ ¸å¿ƒä¸»é¢˜ï¼Œæ¦‚æ‹¬æ‰€æœ‰æ–‡æœ¬çš„å…±åŒç‚¹
- ä¿ç•™ä¸å…¶ä»–ç±»åˆ«çš„åŒºåˆ†åº¦
- ç›´æ¥è¾“å‡ºç±»åï¼Œä¸åŠ ç¼–å·ã€è§£é‡Šå’Œé¢å¤–è¯´æ˜

æ–‡æœ¬æ ·æœ¬ï¼ˆæ¥æºï¼š{sources_str}ï¼‰ï¼š
"""
            # åŠ¨æ€æ·»åŠ æ ·æœ¬æ–‡æœ¬ï¼ˆæ•°é‡æ ¹æ®å®é™…æƒ…å†µå˜åŒ–ï¼‰
            for i, text in enumerate(sample_texts, start=1):
                prompt += f"{i}. {text}\n"  # æ¯æ¡æ–‡æœ¬å•ç‹¬ç¼–å·
            
            # 3. è°ƒç”¨LLMç”Ÿæˆç±»å
            response = client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=TEMPERATURE,  # é™ä½æ¸©åº¦æé«˜ç¨³å®šæ€§ï¼ˆåŸ1.0å®¹æ˜“äº§ç”Ÿä¸ç¨³å®šç»“æœï¼‰
                extra_body={"enable_thinking": False}
            )
            label_name = response.choices[0].message.content.strip()
            
            # 4. æ ¡éªŒæœ€ç»ˆç±»åæœ‰æ•ˆæ€§
            if not label_name:
                raise ValueError("ç”Ÿæˆçš„ç±»åä¸ºç©º")
            if not any('\u4e00' <= c <= '\u9fff' for c in label_name):  # ç¡®ä¿åŒ…å«ä¸­æ–‡
                raise ValueError(f"ç±»åä¸å«æœ‰æ•ˆä¸­æ–‡å­—ç¬¦ï¼š{label_name}")

            print(f"èšç±» {label} å‘½åæˆåŠŸï¼ˆå°è¯•{retry+1}/{max_retries}ï¼‰ï¼š{label_name}")
            return label_name

        except Exception as e:
            # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨å‘½å
            if retry == max_retries - 1:
                print(f"èšç±» {label} å‘½åå¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰ï¼š{str(e)}")
                # ç”Ÿæˆå¤‡ç”¨åç§°ï¼ˆåŸºäºæ–‡æœ¬å“ˆå¸Œï¼‰
                import hashlib
                text_hash = hashlib.md5("".join(paras[:3]).encode()).hexdigest()[:4]
                return f"ä¸»é¢˜_{text_hash}"
            # å¦åˆ™æ‰“å°è­¦å‘Šå¹¶é‡è¯•
            print(f"èšç±» {label} å‘½åå°è¯•{retry+1}/{max_retries}å¤±è´¥ï¼š{str(e)}ï¼Œå°†é‡è¯•...")
            import time
            time.sleep(retry_delay * (retry + 1))  # æŒ‡æ•°é€€é¿å»¶è¿Ÿ

    # æœ€ç»ˆ fallbackï¼ˆç†è®ºä¸Šä¸ä¼šè§¦å‘ï¼‰
    import hashlib
    text_hash = hashlib.md5("".join(paras[:3]).encode()).hexdigest()[:4]
    return f"ä¸»é¢˜_{text_hash}"


# ä¸»å¾ªç¯è°ƒç”¨ï¼ˆæ›¿æ¢åŸforå¾ªç¯ï¼‰
cluster_labels = {}
for label, paras in cluster_texts.items():
    # è·å–è¯¥èšç±»çš„æ¥æºä¿¡æ¯
    cluster_sources_list = cluster_sources.get(label, set())
    # è°ƒç”¨å¸¦é‡è¯•æœºåˆ¶çš„å‘½åå‡½æ•°
    cluster_labels[label] = generate_cluster_label(
        label=label,
        paras=paras,
        sources=cluster_sources_list,
        client=client,
        LLM_MODEL=LLM_MODEL,
        TEMPERATURE=TEMPERATURE
    )

# ========== è¾“å‡ºæœ€ç»ˆæ–‡ä»¶ ==========
print("æ­£åœ¨å†™å…¥ç»“æœæ–‡ä»¶ ...")

# --- 1. æ„å»ºç»“æ„åŒ–æ•°æ® (ç”¨äº JSON å’Œ CSV) ---
output_data = {}
# ä¸º CSV å‡†å¤‡æ•°æ®ï¼Œå­˜å‚¨æ‰€æœ‰æ®µè½åŠå…¶èšç±»ä¿¡æ¯
clustered_csv_data = []

# å¯¹èšç±»æ ‡ç­¾è¿›è¡Œæ’åºï¼Œç¡®ä¿ä¸€è‡´æ€§
sorted_labels = sorted(cluster_labels.keys())

# æ–°å¢ï¼šç”¨äºè®°å½•å·²ä½¿ç”¨çš„å¸¦ç¼–å·ç±»åˆ«åï¼Œé¿å…é‡å¤
used_numbered_names = set()

for i, label in enumerate(sorted_labels, start=1):
    original_name = cluster_labels[label]
    # æ–°å¢ï¼šå¤„ç†åç§°é‡å¤é€»è¾‘ï¼Œè‹¥ original_name å¯¹åº”çš„ numbered_name å·²å­˜åœ¨ï¼Œåˆ™è¿½åŠ åç¼€åŒºåˆ†
    base_numbered_name = f"{i}.{original_name}"
    numbered_name = base_numbered_name
    count = 1
    while numbered_name in used_numbered_names:
        numbered_name = f"{i}.{original_name}_{count}"
        count += 1
    used_numbered_names.add(numbered_name)
    cluster_labels[label] = numbered_name   # æ›´æ–° cluster_labels å­—å…¸

    para_indices = [j for j, l in enumerate(labels) if l == label]

    # åˆå§‹åŒ– JSON è¾“å‡ºçš„åˆ—è¡¨
    output_data[numbered_name] = []

    for j in para_indices:
        para = all_paras[j]
        para_dict = {
            "text": para.get("text", ""),
            "source": para.get("source", "æœªçŸ¥æ¥æº"),
            "page": para.get("page", -1),
            "root_class": numbered_name # ä½¿ç”¨å¸¦ç¼–å·çš„ç±»å
        }
        # æ·»åŠ åˆ° JSON ç»“æ„
        output_data[numbered_name].append(para_dict)

        # ä¸ºèšç±»ç»“æœ CSV å‡†å¤‡æ•°æ®
        csv_row = {
            "cluster_id": label,           # åŸå§‹èšç±»ID (æ•°å­—)
            "root_class": numbered_name,   # å¸¦ç¼–å·å’Œåç§°çš„èšç±»æ ‡ç­¾
            "text": para_dict["text"],
            "source": para_dict["source"],
            "page": para_dict["page"]
        }
        clustered_csv_data.append(csv_row)


# --- 2. å†™å…¥ JSON æ–‡ä»¶ (ä¿æŒåŸæ ·) ---
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)
print(f"èšç±»ç»“æœå·²ä¿å­˜è‡³ (JSON): {OUTPUT_FILE}")

# --- 3. å†™å…¥èšç±»ç»“æœ CSV æ–‡ä»¶ ---
# import csv # å¼•å…¥ csv æ¨¡å—
# CLUSTER_CSV_FILE = OUTPUT_FILE.replace(".json", ".csv") # ç”Ÿæˆ CSV æ–‡ä»¶å
# try:
#     if clustered_csv_data: # ç¡®ä¿æœ‰æ•°æ®å†å†™å…¥
#         with open(CLUSTER_CSV_FILE, "w", newline="", encoding="utf-8") as csvfile:
#             fieldnames = ["cluster_id", "root_class", "text", "source", "page"]
#             writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

#             writer.writeheader()
#             writer.writerows(clustered_csv_data)
#         print(f"èšç±»ç»“æœå·²ä¿å­˜è‡³ (CSV): {CLUSTER_CSV_FILE}")
#     else:
#         print("è­¦å‘Š: æ²¡æœ‰èšç±»æ•°æ®å¯å†™å…¥ CSV æ–‡ä»¶ã€‚")
# except Exception as e:
#     print(f"å†™å…¥èšç±»ç»“æœ CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}")
# --- 3. å†™å…¥èšç±»ç»“æœ Excel æ–‡ä»¶ ---
import pandas as pd

CLUSTER_EXCEL_FILE = OUTPUT_FILE.replace(".json", ".xlsx")  # ç”Ÿæˆ Excel æ–‡ä»¶å

try:
    if clustered_csv_data:  # ç¡®ä¿æœ‰æ•°æ®å†å†™å…¥
        df = pd.DataFrame(clustered_csv_data)
        df.to_excel(CLUSTER_EXCEL_FILE, index=False, engine='openpyxl')
        print(f"èšç±»ç»“æœå·²ä¿å­˜è‡³ (Excel): {CLUSTER_EXCEL_FILE}")
    else:
        print("è­¦å‘Š: æ²¡æœ‰èšç±»æ•°æ®å¯å†™å…¥ Excel æ–‡ä»¶ã€‚")
except Exception as e:
    print(f"å†™å…¥èšç±»ç»“æœ Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# --- 4. å†™å…¥è¢«å¿½ç•¥æ®µè½çš„æ—¥å¿—æ–‡ä»¶ ---
# TXT æ–‡ä»¶ (ä¿æŒåŸæ ·)
if ignored_paragraphs:
    with open(BAD_PARAGRAPH_LOG, "w", encoding="utf-8") as f:
        for item in ignored_paragraphs:
            path = item.get("source", "unknown")
            page = item.get("page", "-")
            para = item.get("text", "")
            f.write(f"{path} - Page {page}\n{para}\n\n")
    print(f"å·²è·³è¿‡æ— æ•ˆæ®µè½ {len(ignored_paragraphs)} æ¡ï¼Œè®°å½•è§ (TXT): {BAD_PARAGRAPH_LOG}")
else:
    print("æ²¡æœ‰æ®µè½è¢«è·³è¿‡ã€‚")

# CSV æ–‡ä»¶ (æ–°å¢)
# IGNORED_CSV_FILE = BAD_PARAGRAPH_LOG.replace(".txt", ".csv") # ç”Ÿæˆ CSV æ–‡ä»¶å
# try:
#     if ignored_paragraphs: # ç¡®ä¿æœ‰æ•°æ®å†å†™å…¥
#         with open(IGNORED_CSV_FILE, "w", newline="", encoding="utf-8") as csvfile:
#             # å®šä¹‰ CSV åˆ—åï¼Œé€šå¸¸ä¸ ignored_paragraphs å­—å…¸çš„é”®å¯¹åº”
#             fieldnames = ["source", "page", "reason", "text"]
#             writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

#             writer.writeheader()
#             # éå† ignored_paragraphs åˆ—è¡¨ï¼Œå†™å…¥æ¯ä¸€è¡Œ
#             for item in ignored_paragraphs:
#                 # ç¡®ä¿å­—æ®µåä¸ fieldnames åŒ¹é…
#                 csv_row = {
#                     "source": item.get("source", "unknown"),
#                     "page": item.get("page", "-"),
#                     "reason": item.get("reason", "unknown"), # ç¡®ä¿ extract_sentences_from_json ä¸­æœ‰è®°å½• reason
#                     "text": item.get("text", "")
#                 }
#                 writer.writerow(csv_row)
#         print(f"å·²è·³è¿‡æ— æ•ˆæ®µè½è®°å½•å·²ä¿å­˜è‡³ (CSV): {IGNORED_CSV_FILE}")
#     else:
#         print("æ²¡æœ‰æ®µè½è¢«è·³è¿‡ï¼Œæ— éœ€ç”Ÿæˆ CSV æ—¥å¿—ã€‚")
# except Exception as e:
#     print(f"å†™å…¥è·³è¿‡æ®µè½ CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# --- å†™å…¥è¢«å¿½ç•¥æ®µè½çš„æ—¥å¿—æ–‡ä»¶ï¼ˆExcel ç‰ˆï¼‰---
IGNORED_EXCEL_FILE = BAD_PARAGRAPH_LOG.replace(".txt", ".xlsx")  # ç”Ÿæˆ Excel æ–‡ä»¶å

try:
    if ignored_paragraphs:  # ç¡®ä¿æœ‰æ•°æ®å†å†™å…¥
        # æ„é€  DataFrame
        data = [
            {
                "source": item.get("source", "unknown"),
                "page": item.get("page", "-"),
                "reason": item.get("reason", "unknown"),
                "text": item.get("text", "")
            }
            for item in ignored_paragraphs
        ]
        df = pd.DataFrame(data)

        # å†™å…¥ Excel
        df.to_excel(IGNORED_EXCEL_FILE, index=False, engine='openpyxl')
        print(f"å·²è·³è¿‡æ— æ•ˆæ®µè½è®°å½•å·²ä¿å­˜è‡³ (Excel): {IGNORED_EXCEL_FILE}")
    else:
        print("æ²¡æœ‰æ®µè½è¢«è·³è¿‡ï¼Œæ— éœ€ç”Ÿæˆ Excel æ—¥å¿—ã€‚")
except Exception as e:
    print(f"å†™å…¥è·³è¿‡æ®µè½ Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}")

print("æ‰€æœ‰æ–‡ä»¶è¾“å‡ºå®Œæˆã€‚")


